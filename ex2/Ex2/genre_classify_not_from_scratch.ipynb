{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from abc import abstractmethod\n",
    "\n",
    "import librosa as librosa\n",
    "import pandas as pd\n",
    "import torch\n",
    "from enum import Enum\n",
    "import typing as tp\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn\n",
    "ROOT = \"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T18:03:01.666982Z",
     "start_time": "2023-05-22T18:02:58.107673900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-22T18:03:01.727629900Z",
     "start_time": "2023-05-22T18:03:01.671504900Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class Genre(Enum):\n",
    "    \"\"\"\n",
    "    This enum class is optional and defined for your convinience, you are not required to use it.\n",
    "    Please use the int labels this enum defines for the corresponding genras in your predictions.\n",
    "    \"\"\"\n",
    "    CLASSICAL: int = 0\n",
    "    HEAVY_ROCK: int = 1\n",
    "    REGGAE: int = 2\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingParameters:\n",
    "    \"\"\"\n",
    "    This dataclass defines a training configuration.\n",
    "    feel free to add/change it as you see fit, do NOT remove the following fields as we will use\n",
    "    them in test time.\n",
    "    If you add additional values to your training configuration please add them in here with\n",
    "    default values (so run won't break when we test this).\n",
    "    \"\"\"\n",
    "    batch_size: int = 32\n",
    "    num_epochs: int = 50\n",
    "    train_json_path: str = \"jsons/train.json\"  # you should use this file path to load your train data\n",
    "    test_json_path: str = \"jsons/test.json\"  # you should use this file path to load your test data\n",
    "    # other training hyper parameters\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OptimizationParameters:\n",
    "    \"\"\"\n",
    "    This dataclass defines optimization related hyper-parameters to be passed to the model.\n",
    "    feel free to add/change it as you see fit.\n",
    "    \"\"\"\n",
    "    learning_rate: float = 0.001\n",
    "\n",
    "\n",
    "class MusicClassifier:\n",
    "    \"\"\"\n",
    "    You should Implement your classifier object here\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, opt_params: OptimizationParameters, **kwargs):\n",
    "        \"\"\"\n",
    "        This defines the classifier object.\n",
    "        - You should defiend your weights and biases as class components here.\n",
    "        - You could use kwargs (dictionary) for any other variables you wish to pass in here.\n",
    "        - You should use `opt_params` for your optimization and you are welcome to experiment\n",
    "        \"\"\"\n",
    "        self.weights = torch.zeros(size=(kwargs[\"num_features\"], len(Genre)), dtype=torch.float32)\n",
    "        self.biases = torch.zeros(size=(1, len(Genre)), dtype=torch.float32)\n",
    "        self.opt_params = opt_params\n",
    "\n",
    "    def exctract_feats(self, wavs: torch.Tensor):\n",
    "        \"\"\"\n",
    "        this function extract features from a given audio.\n",
    "        we will not be observing this method.\n",
    "        \"\"\"\n",
    "        ## features extraction\n",
    "        features_list = []\n",
    "        batch_size = wavs.size()[0]\n",
    "        data = wavs.numpy()\n",
    "        # first family - crossing rate\n",
    "        zero_crossing_rate = librosa.feature.zero_crossing_rate(y=data)\n",
    "        avg_zero_crossing_rate = np.mean(zero_crossing_rate, axis=(1, 2))\n",
    "        std_zero_crossing_rate = np.std(zero_crossing_rate, axis=(1, 2))\n",
    "        features_list.append(torch.tensor(avg_zero_crossing_rate, dtype=torch.float32).view(batch_size, 1))\n",
    "        features_list.append(torch.tensor(std_zero_crossing_rate, dtype=torch.float32).view(batch_size, 1))\n",
    "        # print(\"zero crossing rate: \", zero_crossing_rate.shape, avg_zero_crossing_rate.shape,\n",
    "        #       std_zero_crossing_rate.shape)\n",
    "\n",
    "        # second family - spectral\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=data)\n",
    "        avg_spectral_centroid = np.mean(spectral_centroid, axis=(1, 2))\n",
    "        std_spectral_centroid = np.std(spectral_centroid, axis=(1, 2))\n",
    "        # print(\"spectral centroid: \", spectral_centroid.shape, avg_spectral_centroid.shape,\n",
    "        #       std_spectral_centroid.shape)\n",
    "        features_list.append(torch.tensor(avg_spectral_centroid, dtype=torch.float32).view(batch_size, 1))\n",
    "        features_list.append(torch.tensor(std_spectral_centroid, dtype=torch.float32).view(batch_size, 1))\n",
    "\n",
    "        # third family - spectral contrast\n",
    "        # spectral_contrast = librosa.feature.spectral_contrast(y=data)\n",
    "        # avg_spectral_contrast = np.mean(spectral_contrast, axis=(1, 2))\n",
    "        # std_spectral_contrast = np.std(spectral_contrast, axis=(1, 2))\n",
    "        # # print(\"spectral contrast: \", spectral_contrast.shape, avg_spectral_contrast.shape,\n",
    "        # #       std_spectral_contrast.shape)\n",
    "        # features_list.append(torch.tensor(avg_spectral_contrast, dtype=torch.float32).view(batch_size, 1))\n",
    "        # features_list.append(torch.tensor(std_spectral_contrast, dtype=torch.float32).view(batch_size, 1))\n",
    "\n",
    "        # forth family - log RMS\n",
    "        log_rms = librosa.feature.rms(y=librosa.amplitude_to_db(abs(data)))\n",
    "        avg_log_rms = np.mean(log_rms, axis=(1, 2))\n",
    "        std_log_rms = np.std(log_rms, axis=(1, 2))\n",
    "        # print(\"log RMS: \", log_rms.shape, avg_log_rms.shape, std_log_rms.shape)\n",
    "        features_list.append(torch.tensor(avg_log_rms, dtype=torch.float32).view(batch_size, 1))\n",
    "        features_list.append(torch.tensor(std_log_rms, dtype=torch.float32).view(batch_size, 1))\n",
    "\n",
    "        # # fifth family MFCC features\n",
    "        mfcc = librosa.feature.mfcc(y=data, n_mfcc=13)\n",
    "        # Padding first and second deltas\n",
    "        delta_mfcc = librosa.feature.delta(mfcc)\n",
    "        delta2_mfcc = librosa.feature.delta(mfcc, order=2)\n",
    "        zero_order_mfcc = np.mean(mfcc, axis=2)\n",
    "        first_order_mfcc = np.mean(delta_mfcc, axis=2)\n",
    "        second_order_mfcc = np.mean(delta2_mfcc, axis=2)\n",
    "\n",
    "        # print(\"MFCC: \", mfcc.shape, zero_order_mfcc.shape, first_order_mfcc.shape, second_order_mfcc.shape)\n",
    "        features_list.append(torch.tensor(zero_order_mfcc, dtype=torch.float32).view(batch_size, 13))\n",
    "        features_list.append(torch.tensor(first_order_mfcc, dtype=torch.float32).view(batch_size, 13))\n",
    "        features_list.append(torch.tensor(second_order_mfcc, dtype=torch.float32).view(batch_size, 13))\n",
    "\n",
    "        # sixth family - chroma\n",
    "        # chroma = librosa.feature.chroma_stft(y=data)\n",
    "        # avg_chroma = np.mean(chroma, axis=(1, 2))\n",
    "        # std_chroma = np.std(chroma, axis=(1, 2))\n",
    "        # # print(\"chroma: \", chroma.shape, avg_chroma.shape, std_chroma.shape)\n",
    "        # features_list.append(torch.tensor(avg_chroma, dtype=torch.float32).view(batch_size, 1))\n",
    "        # features_list.append(torch.tensor(std_chroma, dtype=torch.float32).view(batch_size, 1))\n",
    "\n",
    "        # seventh family - tonnetz\n",
    "        # tonnetz = librosa.feature.tonnetz(y=data)\n",
    "        # avg_tonnetz = np.mean(tonnetz, axis=(1, 2))\n",
    "        # std_tonnetz = np.std(tonnetz, axis=(1, 2))\n",
    "        # # print(\"tonnetz: \", tonnetz.shape, avg_tonnetz.shape, std_tonnetz.shape)\n",
    "        # features_list.append(torch.tensor(avg_tonnetz, dtype=torch.float32).view(batch_size, 1))\n",
    "        # features_list.append(torch.tensor(std_tonnetz, dtype=torch.float32).view(batch_size, 1))\n",
    "\n",
    "        # eigth - spectral flatness, rolloff, bandwidth\n",
    "        spectral_flatness = librosa.feature.spectral_flatness(y=data)\n",
    "        avg_spectral_flatness = np.mean(spectral_flatness, axis=(1, 2))\n",
    "        std_spectral_flatness = np.std(spectral_flatness, axis=(1, 2))\n",
    "        # print(\"spectral flatness: \", spectral_flatness.shape, avg_spectral_flatness.shape,\n",
    "        #       std_spectral_flatness.shape)\n",
    "        features_list.append(torch.tensor(avg_spectral_flatness, dtype=torch.float32).view(batch_size, 1))\n",
    "        features_list.append(torch.tensor(std_spectral_flatness, dtype=torch.float32).view(batch_size, 1))\n",
    "\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=data)\n",
    "        avg_spectral_rolloff = np.mean(spectral_rolloff, axis=(1, 2))\n",
    "        std_spectral_rolloff = np.std(spectral_rolloff, axis=(1, 2))\n",
    "        # print(\"spectral rolloff: \", spectral_rolloff.shape, avg_spectral_rolloff.shape,\n",
    "        #       std_spectral_rolloff.shape)\n",
    "        features_list.append(torch.tensor(avg_spectral_rolloff, dtype=torch.float32).view(batch_size, 1))\n",
    "        features_list.append(torch.tensor(std_spectral_rolloff, dtype=torch.float32).view(batch_size, 1))\n",
    "\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=data)\n",
    "        avg_spectral_bandwidth = np.mean(spectral_bandwidth, axis=(1, 2))\n",
    "        std_spectral_bandwidth = np.std(spectral_bandwidth, axis=(1, 2))\n",
    "        # print(\"spectral bandwidth: \", spectral_bandwidth.shape, avg_spectral_bandwidth.shape,\n",
    "        #       std_spectral_bandwidth.shape)\n",
    "        features_list.append(torch.tensor(avg_spectral_bandwidth, dtype=torch.float32).view(batch_size, 1))\n",
    "        features_list.append(torch.tensor(std_spectral_bandwidth, dtype=torch.float32).view(batch_size, 1))\n",
    "\n",
    "\n",
    "        features = torch.cat(features_list, dim=1)\n",
    "\n",
    "        return features\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + torch.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(z):\n",
    "        exps = torch.exp(z-torch.max(z))\n",
    "        return  exps / torch.sum(exps, dim=1, keepdim=True)\n",
    "\n",
    "    def forward(self, feats: torch.Tensor) -> tp.Any:\n",
    "        \"\"\"\n",
    "        this function performs a forward pass throuh the model, outputting scores for every class.\n",
    "        feats: batch of extracted faetures\n",
    "        \"\"\"\n",
    "        x = torch.mm(feats, self.weights) + self.biases\n",
    "        transformed_x = x - torch.max(x, dim=1, keepdim=True)[0]\n",
    "        return torch.softmax(transformed_x, dim=1)\n",
    "\n",
    "    def backward(self, feats: torch.Tensor, output_scores: torch.Tensor, labels: torch.Tensor):\n",
    "        \"\"\"\n",
    "        this function should perform a backward pass through the model.\n",
    "        - calculate loss\n",
    "        - calculate gradients\n",
    "        - update gradients using SGD\n",
    "\n",
    "        Note: in practice - the optimization process is usually external to the model.\n",
    "        We thought it may result in less coding needed if you are to apply it here, hence\n",
    "        OptimizationParameters are passed to the initialization function\n",
    "        \"\"\"\n",
    "\n",
    "        num_examples = feats.size()[0]\n",
    "\n",
    "        # Compute gradients\n",
    "        d_scores = output_scores.clone()\n",
    "        d_scores[torch.arange(num_examples), torch.argmax(labels, dim=1)] -= 1\n",
    "        d_scores /= num_examples\n",
    "\n",
    "        d_weights = torch.matmul(feats.T, d_scores)\n",
    "        d_biases = torch.sum(d_scores, dim=0, keepdim=True)\n",
    "\n",
    "        # d_weights = torch.matmul(feats.T, output_scores-labels)/num_examples\n",
    "        # d_biases = torch.sum(output_scores-labels, dim=0, keepdim=True)/num_examples\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights -= self.opt_params.learning_rate * d_weights\n",
    "        self.biases -= self.opt_params.learning_rate * d_biases\n",
    "\n",
    "        # Calculate the loss\n",
    "        # loss = self.calculate_loss(output_scores, labels)\n",
    "        loss = self.calculate_loss(output_scores, labels)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def calculate_loss(self, output_scores, labels):\n",
    "        # num_examples = output_scores.shape[0]\n",
    "        # correct_logprobs = -torch.log(output_scores[range(num_examples), labels])\n",
    "        # data_loss = torch.sum(correct_logprobs) / num_examples\n",
    "        return torch.nn.functional.binary_cross_entropy(output_scores, labels)\n",
    "\n",
    "    def get_weights_and_biases(self):\n",
    "        \"\"\"\n",
    "        This function returns the weights and biases associated with this model object,\n",
    "        should return a tuple: (weights, biases)\n",
    "        \"\"\"\n",
    "        # This function returns the weights and biases associated with this model object,\n",
    "        # should return a tuple: (weights, biases)\n",
    "        return self.weights, self.biases\n",
    "\n",
    "    def classify(self, wavs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        this method should recieve a torch.Tensor of shape [batch, channels, time] (float tensor)\n",
    "        and a output batch of corresponding labels [B, 1] (integer tensor)\n",
    "        \"\"\"\n",
    "        feats = self.exctract_feats(wavs)\n",
    "        outputs = self.forward(feats)\n",
    "        _, predicted = torch.max(outputs.data, dim=1)\n",
    "        return predicted\n",
    "\n",
    "    def train(self, training_parameters: TrainingParameters, data=None):\n",
    "\n",
    "        num_epochs = training_parameters.num_epochs\n",
    "        batch_size = training_parameters.batch_size\n",
    "        if data is None:\n",
    "            train_data = ClassifierHandler.load_wav_files(training_parameters.train_json_path)\n",
    "            test_data = ClassifierHandler.load_wav_files(training_parameters.test_json_path)\n",
    "        else:\n",
    "            train_data = data\n",
    "            test_data = data\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "            audio = torch.tensor(train_data['audio'].tolist(), dtype=torch.float32)\n",
    "            labels = torch.tensor(train_data['label'].tolist(), dtype=torch.int8)\n",
    "            for i in range(0, len(train_data), batch_size):\n",
    "                data = audio[i: i + batch_size]\n",
    "                one_hot_labels = torch.eye(3)[labels[i:i + batch_size].to(int)]\n",
    "                features = self.exctract_feats(data)\n",
    "                output_scores = self.forward(features)\n",
    "                loss = self.backward(features, output_scores, one_hot_labels)\n",
    "                if i / batch_size % 1 == 0:\n",
    "                    print('Epoch: {}, batch: {} / {}, loss: {}'.format(epoch + 1, i, len(train_data), loss))\n",
    "\n",
    "            self.test(torch.tensor(test_data['audio'].tolist(), dtype=torch.float32), test_data['label'])\n",
    "\n",
    "    def test(self, test_data, test_lables):\n",
    "        predictions = self.classify(test_data)\n",
    "        loss = accuracy_score(predictions, torch.tensor(test_lables.tolist()))\n",
    "        print('Accuracy: {}'.format(loss))\n",
    "\n",
    "\n",
    "class ClassifierHandler:\n",
    "    @staticmethod\n",
    "    def train_new_model(training_parameters: TrainingParameters) -> MusicClassifier:\n",
    "        \"\"\"\n",
    "        This function should create a new 'MusicClassifier' object and train it from scratch.\n",
    "        You could program your training loop / training manager as you see fit.\n",
    "        \"\"\"\n",
    "        # should initialize a complete training (loading data, init model, start training/fitting) and\n",
    "        # to save weights/other to model files directory. This function should recieve a TrainingParameters\n",
    "        # dataclass\n",
    "        # object and perform training accordingly, see code documentation for further details.\n",
    "        # load the data from the json files\n",
    "\n",
    "        opti_params = OptimizationParameters()\n",
    "        music_classifier = MusicClassifier(opti_params, **{'num_features': 51})\n",
    "\n",
    "        music_classifier.train(training_parameters)\n",
    "\n",
    "        # save the model using pickle\n",
    "        pickle.dump(music_classifier, open('model2.pkl', 'wb'))\n",
    "\n",
    "        return music_classifier\n",
    "\n",
    "    @staticmethod\n",
    "    def get_pretrained_model() -> MusicClassifier:\n",
    "        \"\"\"\n",
    "        This function should construct a 'MusicClassifier' object, load it's trained weights /\n",
    "        hyperparameters and return the loaded model\n",
    "        \"\"\"\n",
    "        # should load a model from model files directory. This function should return a MusicClassifier object\n",
    "        # with loaded weights/other.\n",
    "        model = pickle.load(open('model.pkl', 'rb'))\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def load_wav_files(json_file_path):\n",
    "        # Read the JSON file\n",
    "        with open(json_file_path) as json_file:\n",
    "            data = json.load(json_file)\n",
    "\n",
    "        # Create an empty DataFrame\n",
    "        df = pd.DataFrame(columns=['label', 'audio', 'sr'])\n",
    "\n",
    "        # Iterate over first 50 items in the JSON data\n",
    "        for item in data:\n",
    "            path = item['path']\n",
    "            label = item['label']\n",
    "            label = str.replace(label, '-', '_')\n",
    "            label = Genre[label.upper()].value\n",
    "            # Load the audio file using librosa\n",
    "            audio, sr = librosa.load(path, sr=None)\n",
    "\n",
    "            # Append the path, label, and audio to the DataFrameu78yt6r5fe4\n",
    "            df = df.append({'label': label, 'audio': audio, 'sr': sr}, ignore_index=True)\n",
    "\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1079, 266112])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_10300\\878704945.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  wavs = torch.tensor(train_data['audio'])\n"
     ]
    }
   ],
   "source": [
    "train_data = ClassifierHandler.load_wav_files(\"jsons/train.json\")\n",
    "wavs = torch.tensor(train_data['audio'])\n",
    "print(wavs.size())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T18:03:57.103598500Z",
     "start_time": "2023-05-22T18:03:01.735236300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def exctract_feats(wavs: torch.Tensor):\n",
    "        \"\"\"\n",
    "        this function extract features from a given audio.\n",
    "        we will not be observing this method.\n",
    "        \"\"\"\n",
    "        ## features extraction\n",
    "        features_list = []\n",
    "        batch_size = wavs.size()[0]\n",
    "        data = wavs.numpy()\n",
    "        # first family - crossing rate\n",
    "        zero_crossing_rate = librosa.feature.zero_crossing_rate(y=data)\n",
    "        avg_zero_crossing_rate = np.mean(zero_crossing_rate, axis=(1, 2))\n",
    "        std_zero_crossing_rate = np.std(zero_crossing_rate, axis=(1, 2))\n",
    "        features_list.append(torch.tensor(avg_zero_crossing_rate, dtype=torch.float32).view(batch_size, 1))\n",
    "        features_list.append(torch.tensor(std_zero_crossing_rate, dtype=torch.float32).view(batch_size, 1))\n",
    "        # print(\"zero crossing rate: \", zero_crossing_rate.shape, avg_zero_crossing_rate.shape,\n",
    "        #       std_zero_crossing_rate.shape)\n",
    "\n",
    "        # second family - spectral\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=data)\n",
    "        avg_spectral_centroid = np.mean(spectral_centroid, axis=(1, 2))\n",
    "        std_spectral_centroid = np.std(spectral_centroid, axis=(1, 2))\n",
    "        # print(\"spectral centroid: \", spectral_centroid.shape, avg_spectral_centroid.shape,\n",
    "        #       std_spectral_centroid.shape)\n",
    "        features_list.append(torch.tensor(avg_spectral_centroid, dtype=torch.float32).view(batch_size, 1))\n",
    "        features_list.append(torch.tensor(std_spectral_centroid, dtype=torch.float32).view(batch_size, 1))\n",
    "\n",
    "        # third family - spectral contrast\n",
    "        # spectral_contrast = librosa.feature.spectral_contrast(y=data)\n",
    "        # avg_spectral_contrast = np.mean(spectral_contrast, axis=(1, 2))\n",
    "        # std_spectral_contrast = np.std(spectral_contrast, axis=(1, 2))\n",
    "        # # print(\"spectral contrast: \", spectral_contrast.shape, avg_spectral_contrast.shape,\n",
    "        # #       std_spectral_contrast.shape)\n",
    "        # features_list.append(torch.tensor(avg_spectral_contrast, dtype=torch.float32).view(batch_size, 1))\n",
    "        # features_list.append(torch.tensor(std_spectral_contrast, dtype=torch.float32).view(batch_size, 1))\n",
    "\n",
    "        # forth family - log RMS\n",
    "        log_rms = librosa.feature.rms(y=librosa.amplitude_to_db(abs(data)))\n",
    "        avg_log_rms = np.mean(log_rms, axis=(1, 2))\n",
    "        std_log_rms = np.std(log_rms, axis=(1, 2))\n",
    "        # print(\"log RMS: \", log_rms.shape, avg_log_rms.shape, std_log_rms.shape)\n",
    "        features_list.append(torch.tensor(avg_log_rms, dtype=torch.float32).view(batch_size, 1))\n",
    "        features_list.append(torch.tensor(std_log_rms, dtype=torch.float32).view(batch_size, 1))\n",
    "\n",
    "        # # fifth family MFCC features\n",
    "        mfcc = librosa.feature.mfcc(y=data, n_mfcc=13)\n",
    "        # Padding first and second deltas\n",
    "        delta_mfcc = librosa.feature.delta(mfcc)\n",
    "        delta2_mfcc = librosa.feature.delta(mfcc, order=2)\n",
    "        zero_order_mfcc = np.mean(mfcc, axis=2)\n",
    "        first_order_mfcc = np.mean(delta_mfcc, axis=2)\n",
    "        second_order_mfcc = np.mean(delta2_mfcc, axis=2)\n",
    "\n",
    "        # print(\"MFCC: \", mfcc.shape, zero_order_mfcc.shape, first_order_mfcc.shape, second_order_mfcc.shape)\n",
    "        features_list.append(torch.tensor(zero_order_mfcc, dtype=torch.float32).view(batch_size, 13))\n",
    "        features_list.append(torch.tensor(first_order_mfcc, dtype=torch.float32).view(batch_size, 13))\n",
    "        features_list.append(torch.tensor(second_order_mfcc, dtype=torch.float32).view(batch_size, 13))\n",
    "\n",
    "        # sixth family - chroma\n",
    "        # chroma = librosa.feature.chroma_stft(y=data)\n",
    "        # avg_chroma = np.mean(chroma, axis=(1, 2))\n",
    "        # std_chroma = np.std(chroma, axis=(1, 2))\n",
    "        # # print(\"chroma: \", chroma.shape, avg_chroma.shape, std_chroma.shape)\n",
    "        # features_list.append(torch.tensor(avg_chroma, dtype=torch.float32).view(batch_size, 1))\n",
    "        # features_list.append(torch.tensor(std_chroma, dtype=torch.float32).view(batch_size, 1))\n",
    "\n",
    "        # seventh family - tonnetz\n",
    "        # tonnetz = librosa.feature.tonnetz(y=data)\n",
    "        # avg_tonnetz = np.mean(tonnetz, axis=(1, 2))\n",
    "        # std_tonnetz = np.std(tonnetz, axis=(1, 2))\n",
    "        # # print(\"tonnetz: \", tonnetz.shape, avg_tonnetz.shape, std_tonnetz.shape)\n",
    "        # features_list.append(torch.tensor(avg_tonnetz, dtype=torch.float32).view(batch_size, 1))\n",
    "        # features_list.append(torch.tensor(std_tonnetz, dtype=torch.float32).view(batch_size, 1))\n",
    "\n",
    "        # eigth - spectral flatness, rolloff, bandwidth\n",
    "        spectral_flatness = librosa.feature.spectral_flatness(y=data)\n",
    "        avg_spectral_flatness = np.mean(spectral_flatness, axis=(1, 2))\n",
    "        std_spectral_flatness = np.std(spectral_flatness, axis=(1, 2))\n",
    "        # print(\"spectral flatness: \", spectral_flatness.shape, avg_spectral_flatness.shape,\n",
    "        #       std_spectral_flatness.shape)\n",
    "        features_list.append(torch.tensor(avg_spectral_flatness, dtype=torch.float32).view(batch_size, 1))\n",
    "        features_list.append(torch.tensor(std_spectral_flatness, dtype=torch.float32).view(batch_size, 1))\n",
    "\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=data)\n",
    "        avg_spectral_rolloff = np.mean(spectral_rolloff, axis=(1, 2))\n",
    "        std_spectral_rolloff = np.std(spectral_rolloff, axis=(1, 2))\n",
    "        # print(\"spectral rolloff: \", spectral_rolloff.shape, avg_spectral_rolloff.shape,\n",
    "        #       std_spectral_rolloff.shape)\n",
    "        features_list.append(torch.tensor(avg_spectral_rolloff, dtype=torch.float32).view(batch_size, 1))\n",
    "        features_list.append(torch.tensor(std_spectral_rolloff, dtype=torch.float32).view(batch_size, 1))\n",
    "\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=data)\n",
    "        avg_spectral_bandwidth = np.mean(spectral_bandwidth, axis=(1, 2))\n",
    "        std_spectral_bandwidth = np.std(spectral_bandwidth, axis=(1, 2))\n",
    "        # print(\"spectral bandwidth: \", spectral_bandwidth.shape, avg_spectral_bandwidth.shape,\n",
    "        #       std_spectral_bandwidth.shape)\n",
    "        features_list.append(torch.tensor(avg_spectral_bandwidth, dtype=torch.float32).view(batch_size, 1))\n",
    "        features_list.append(torch.tensor(std_spectral_bandwidth, dtype=torch.float32).view(batch_size, 1))\n",
    "\n",
    "\n",
    "        features = torch.cat(features_list, dim=1)\n",
    "\n",
    "        return features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T18:10:38.619647700Z",
     "start_time": "2023-05-22T18:10:38.596501100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  0\n",
      "features:  torch.Size([32, 51])\n",
      "batch:  32\n",
      "features:  torch.Size([64, 51])\n",
      "batch:  64\n",
      "features:  torch.Size([96, 51])\n",
      "batch:  96\n",
      "features:  torch.Size([128, 51])\n",
      "batch:  128\n",
      "features:  torch.Size([160, 51])\n",
      "batch:  160\n",
      "features:  torch.Size([192, 51])\n",
      "batch:  192\n",
      "features:  torch.Size([224, 51])\n",
      "batch:  224\n",
      "features:  torch.Size([256, 51])\n",
      "batch:  256\n",
      "features:  torch.Size([288, 51])\n",
      "batch:  288\n",
      "features:  torch.Size([320, 51])\n",
      "batch:  320\n",
      "features:  torch.Size([352, 51])\n",
      "batch:  352\n",
      "features:  torch.Size([384, 51])\n",
      "batch:  384\n",
      "features:  torch.Size([416, 51])\n",
      "batch:  416\n",
      "features:  torch.Size([448, 51])\n",
      "batch:  448\n",
      "features:  torch.Size([480, 51])\n",
      "batch:  480\n",
      "features:  torch.Size([512, 51])\n",
      "batch:  512\n",
      "features:  torch.Size([544, 51])\n",
      "batch:  544\n",
      "features:  torch.Size([576, 51])\n",
      "batch:  576\n",
      "features:  torch.Size([608, 51])\n",
      "batch:  608\n",
      "features:  torch.Size([640, 51])\n",
      "batch:  640\n",
      "features:  torch.Size([672, 51])\n",
      "batch:  672\n",
      "features:  torch.Size([704, 51])\n",
      "batch:  704\n",
      "features:  torch.Size([736, 51])\n",
      "batch:  736\n",
      "features:  torch.Size([768, 51])\n",
      "batch:  768\n",
      "features:  torch.Size([800, 51])\n",
      "batch:  800\n",
      "features:  torch.Size([832, 51])\n",
      "batch:  832\n",
      "features:  torch.Size([864, 51])\n",
      "batch:  864\n",
      "features:  torch.Size([896, 51])\n",
      "batch:  896\n",
      "features:  torch.Size([928, 51])\n",
      "batch:  928\n",
      "features:  torch.Size([960, 51])\n",
      "batch:  960\n",
      "features:  torch.Size([992, 51])\n",
      "batch:  992\n",
      "features:  torch.Size([1024, 51])\n",
      "batch:  1024\n",
      "features:  torch.Size([1056, 51])\n",
      "batch:  1056\n",
      "features:  torch.Size([1079, 51])\n"
     ]
    }
   ],
   "source": [
    "# extract features by batches\n",
    "for batch in range(0, len(train_data), 32):\n",
    "    print(\"batch: \", batch)\n",
    "    batch_data = wavs[batch:batch + 32]\n",
    "    batch_features = exctract_feats(batch_data)\n",
    "    if batch == 0:\n",
    "        features = batch_features\n",
    "    else:\n",
    "        features = torch.cat((features, batch_features), dim=0)\n",
    "    print(\"features: \", features.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T18:18:34.177510900Z",
     "start_time": "2023-05-22T18:15:12.646277300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "labels = torch.tensor(train_data['label'].tolist(), dtype=torch.int8)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T18:22:44.504177600Z",
     "start_time": "2023-05-22T18:22:44.483705800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        72\n",
      "           1       0.96      0.97      0.96        66\n",
      "           2       0.97      0.96      0.97        78\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Accuracy:  0.9768518518518519\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# split your data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# create an instance of the Logistic Regression model\n",
    "# we're using a simple linear logistic regression, so we'll use 'liblinear' solver\n",
    "# 'liblinear' solver is good for small datasets and binary classification\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# print out a report on how the model did\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "#print the accuracy\n",
    "print(\"Accuracy: \", model.score(X_test, y_test))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T18:24:44.672340400Z",
     "start_time": "2023-05-22T18:24:44.522967700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted_coef len:  51\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 720x360 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# visualize which features were most important to the model\n",
    "# this is a good way to see if there are any features that you can remove\n",
    "# to improve your model\n",
    "def visualize_features(model:sklearn.linear_model.LogisticRegression, feature_names:list):\n",
    "    # get the coefficients of the model\n",
    "    coef = model.coef_[0]\n",
    "\n",
    "    # get the absolute value of the coefficients\n",
    "    abs_coef = np.abs(coef)\n",
    "\n",
    "    # sort the coefficients from highest to lowest\n",
    "    sorted_coef = np.sort(abs_coef)[::-1]\n",
    "    print(\"sorted_coef len: \", len(sorted_coef))\n",
    "\n",
    "    # get the indexes of the coefficients from highest to lowest\n",
    "    sorted_coef_index = np.argsort(abs_coef)[::-1]\n",
    "\n",
    "    # match the coefficients to their feature names, and sort them from highest to lowest\n",
    "    sorted_coef_names = []\n",
    "    for i in sorted_coef_index:\n",
    "        sorted_coef_names.append(feature_names[i])\n",
    "\n",
    "    # create a figure\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # plot the features and their coefficients\n",
    "    plt.bar(sorted_coef_names, sorted_coef)\n",
    "\n",
    "    # rotate the ticks on the x-axis\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_features(model, list(np.arange(0, 51)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T18:30:34.799416800Z",
     "start_time": "2023-05-22T18:30:34.628436Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "class LogisticRegressor:\n",
    "    def __init__(self, input_dim: int, num_classes: int):\n",
    "        self.weights = torch.randn(input_dim, num_classes, requires_grad=True)\n",
    "        self.bias = torch.randn(num_classes, requires_grad=True)\n",
    "\n",
    "    def forward(self, feats: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.mm(feats, self.weights) + self.bias\n",
    "\n",
    "    def backward(self, feats: torch.Tensor, output_scores: torch.Tensor, labels: torch.Tensor):\n",
    "        loss = torch.nn.functional.cross_entropy(output_scores, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.weights -= 0.01 * self.weights.grad\n",
    "            self.bias -= 0.01 * self.bias.grad\n",
    "\n",
    "            # zero the gradients after updating\n",
    "            self.weights.grad.zero_()\n",
    "            self.bias.grad.zero_()\n",
    "\n",
    "    def extract_feats(self, wavs: torch.Tensor):\n",
    "        # insert your feature extraction logic here\n",
    "        pass\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T18:48:40.217258300Z",
     "start_time": "2023-05-22T18:48:40.192490800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "def train_and_test_model(features: np.ndarray, labels: np.ndarray, epochs: int=100):\n",
    "    # convert features and labels to torch tensors\n",
    "    features = torch.tensor(features, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    # split the features and labels into a training set and a testing set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # get the number of features from the feature data\n",
    "    input_dim = X_train.shape[1]\n",
    "\n",
    "    # get the number of unique classes from the labels\n",
    "    num_classes = len(torch.unique(labels))\n",
    "\n",
    "    # create the model\n",
    "    model = LogisticRegressor(input_dim, num_classes)\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(epochs):\n",
    "        # forward pass\n",
    "        output_scores = model.forward(X_train)\n",
    "\n",
    "        # backward pass and optimization\n",
    "        model.backward(X_train, output_scores, y_train)\n",
    "\n",
    "        # print loss every 10 epochs\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            loss = torch.nn.functional.cross_entropy(output_scores, y_train)\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
    "\n",
    "    # testing the model\n",
    "    test_output_scores = model.forward(X_test)\n",
    "    _, predicted = torch.max(test_output_scores.data, 1)\n",
    "    correct = (predicted == y_test).sum().item()\n",
    "    test_accuracy = correct / len(y_test)\n",
    "    print(f'Test accuracy: {test_accuracy * 100:.2f}%')\n",
    "\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T18:48:41.837729600Z",
     "start_time": "2023-05-22T18:48:41.777345500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_10300\\3220998991.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  features = torch.tensor(features, dtype=torch.float32)\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_10300\\3220998991.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 30840.791015625\n",
      "Epoch 20/100, Loss: 54237.51953125\n",
      "Epoch 30/100, Loss: 33636.95703125\n",
      "Epoch 40/100, Loss: 31511.73828125\n",
      "Epoch 50/100, Loss: 2131.889404296875\n",
      "Epoch 60/100, Loss: 4846.70458984375\n",
      "Epoch 70/100, Loss: 6245.89453125\n",
      "Epoch 80/100, Loss: 3658.55029296875\n",
      "Epoch 90/100, Loss: 4493.28759765625\n",
      "Epoch 100/100, Loss: 1713.122802734375\n",
      "Test accuracy: 90.28%\n"
     ]
    },
    {
     "data": {
      "text/plain": "<__main__.LogisticRegressor at 0x1f00755bd00>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = train_and_test_model(features, labels, epochs=100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T18:48:44.466262100Z",
     "start_time": "2023-05-22T18:48:44.216115500Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
