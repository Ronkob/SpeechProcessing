{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from abc import abstractmethod\n",
    "\n",
    "import librosa as librosa\n",
    "import pandas as pd\n",
    "import torch\n",
    "from enum import Enum\n",
    "import typing as tp\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn\n",
    "from torch.nn.functional import cross_entropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ex2.Ex2.genre_classifier import Genre\n",
    "import json\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from multiprocessing import Pool\n",
    "ROOT = \"\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T19:10:50.973347Z",
     "start_time": "2023-05-22T19:10:47.597799500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def extract_features(wavs):\n",
    "    all_features = []\n",
    "\n",
    "    for wav in wavs:\n",
    "        # Calculate MFCC\n",
    "        mfccs = librosa.feature.mfcc(y=wav, sr=22050, n_mfcc=13)\n",
    "\n",
    "        # Calculate spectral contrast\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(y=wav, sr=22050)\n",
    "\n",
    "        # Calculate chroma features\n",
    "        chroma_stft = librosa.feature.chroma_stft(y=wav, sr=22050)\n",
    "\n",
    "        # Calculate tonnetz\n",
    "        tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(wav), sr=22050)\n",
    "\n",
    "        # Stack and transpose for correct dimensions\n",
    "        features = np.vstack([mfccs, spectral_contrast, chroma_stft, tonnetz]).T\n",
    "\n",
    "        # Calculate mean and standard deviation for each feature\n",
    "        feature_stats = np.hstack([np.mean(features, axis=0), np.std(features, axis=0)])\n",
    "\n",
    "        all_features.append(feature_stats)\n",
    "\n",
    "    return np.array(all_features)\n",
    "\n",
    "def softmax(z):\n",
    "    e_z = torch.exp(z - torch.max(z))\n",
    "    return e_z / e_z.sum(axis=1, keepdims=True)\n",
    "\n",
    "class LogisticRegressor:\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        self.weights = torch.random.randn(input_dim, num_classes)\n",
    "        self.bias = torch.random.randn(num_classes)\n",
    "\n",
    "    def forward(self, feats):\n",
    "        z = torch.mm(feats, self.weights) + self.bias\n",
    "        return softmax(z)\n",
    "\n",
    "    def compute_gradients(self, feats, output_scores, labels):\n",
    "        num_samples = feats.shape[0]\n",
    "        output_scores[range(num_samples), labels] -= 1\n",
    "        output_scores /= num_samples\n",
    "\n",
    "        grad_weights = torch.mm(feats.T, output_scores)\n",
    "        grad_bias = torch.sum(output_scores, dim=0)\n",
    "\n",
    "        return grad_weights, grad_bias\n",
    "\n",
    "    def update_weights(self, grad_weights, grad_bias, lr):\n",
    "        self.weights -= lr * grad_weights\n",
    "        self.bias -= lr * grad_bias\n",
    "\n",
    "    def train(self, feats, labels, epochs=100, lr=0.01):\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch: {epoch}\")\n",
    "            output_scores = self.forward(feats)\n",
    "\n",
    "            grad_weights, grad_bias = self.compute_gradients(feats, output_scores.copy(), labels)\n",
    "\n",
    "            self.update_weights(grad_weights, grad_bias, lr)\n",
    "            # print output scores and loss\n",
    "            if epoch % 1 == 0:\n",
    "                loss = cross_entropy(output_scores, labels)\n",
    "                print(f\"Epoch: {epoch}, Loss: {loss}\")\n",
    "\n",
    "    def predict(self, feats):\n",
    "        output_scores = self.forward(feats)\n",
    "        return torch.argmax(output_scores, dim=1)\n",
    "\n",
    "def train_and_test_model(wavs, labels, features=None, epochs=100, lr=0.01):\n",
    "    # extract features\n",
    "    if features is None:\n",
    "        features = extract_features(wavs)\n",
    "\n",
    "    # normalize features\n",
    "    features = (features - np.mean(features, axis=0)) / np.std(features, axis=0)\n",
    "\n",
    "    # split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # create and train the model\n",
    "    model = LogisticRegressor(X_train.shape[1], len(np.unique(labels)))\n",
    "    model.train(X_train, y_train, epochs, lr)\n",
    "\n",
    "    # test the model\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = np.mean(predictions == y_test)\n",
    "    print(f\"Test accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T19:44:31.858902900Z",
     "start_time": "2023-05-22T19:44:31.845996Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "\n",
    "# Function to load a single wave file\n",
    "def load_wav_file(item):\n",
    "    path = item['path']\n",
    "    label = item['label']\n",
    "    label = str.replace(label, '-', '_')\n",
    "    label = Genre[label.upper()].value\n",
    "    audio, sr = librosa.load(path, sr=None)\n",
    "    return label, audio, sr\n",
    "\n",
    "def load_wav_files_parallel(json_file_path):\n",
    "    # Read the JSON file\n",
    "    with open(json_file_path) as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    # Use a multiprocessing Pool to load the wave files in parallel\n",
    "    with Pool() as p:\n",
    "        results = p.map(load_wav_file, data)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    df = pd.DataFrame(results, columns=['label', 'audio', 'sr'])\n",
    "\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def load_wav_files(json_file_path):\n",
    "    # Read the JSON file\n",
    "    with open(json_file_path) as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    # Create an empty DataFrame\n",
    "    df = pd.DataFrame(columns=['label', 'audio', 'sr'])\n",
    "\n",
    "    # Iterate over first 50 items in the JSON data\n",
    "    for item in data:\n",
    "        path = item['path']\n",
    "        label = item['label']\n",
    "        label = str.replace(label, '-', '_')\n",
    "        label = Genre[label.upper()].value\n",
    "        # Load the audio file using librosa\n",
    "        audio, sr = librosa.load(path, sr=None)\n",
    "\n",
    "        # Append the path, label, and audio to the DataFrameu78yt6r5fe4\n",
    "        df = df.append({'label': label, 'audio': audio, 'sr': sr}, ignore_index=True)\n",
    "\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T19:28:17.275273700Z",
     "start_time": "2023-05-22T19:28:17.253545500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "train_data = load_wav_files(ROOT + 'jsons/train.json')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T19:13:02.089014500Z",
     "start_time": "2023-05-22T19:12:51.163763Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "test_data = load_wav_files(ROOT + 'jsons/test.json')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T19:11:44.058470200Z",
     "start_time": "2023-05-22T19:11:41.859681600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "data = np.array(train_data['audio'])\n",
    "labels = np.array(train_data['label'], dtype=int)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T19:37:41.472960500Z",
     "start_time": "2023-05-22T19:37:41.450925Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def exctract_feats(wavs: torch.Tensor):\n",
    "    \"\"\"\n",
    "    this function extract features from a given audio.\n",
    "    we will not be observing this method.\n",
    "    \"\"\"\n",
    "    ## features extraction\n",
    "    features_list = []\n",
    "    batch_size = wavs.size()[0]\n",
    "    data = wavs.numpy()\n",
    "    # first family - crossing rate\n",
    "    zero_crossing_rate = librosa.feature.zero_crossing_rate(y=data)\n",
    "    avg_zero_crossing_rate = np.mean(zero_crossing_rate, axis=(1, 2))\n",
    "    std_zero_crossing_rate = np.std(zero_crossing_rate, axis=(1, 2))\n",
    "    features_list.append(torch.tensor(avg_zero_crossing_rate, dtype=torch.float32).view(batch_size, 1))\n",
    "    features_list.append(torch.tensor(std_zero_crossing_rate, dtype=torch.float32).view(batch_size, 1))\n",
    "    # print(\"zero crossing rate: \", zero_crossing_rate.shape, avg_zero_crossing_rate.shape,\n",
    "    #       std_zero_crossing_rate.shape)\n",
    "\n",
    "    # second family - spectral\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=data)\n",
    "    avg_spectral_centroid = np.mean(spectral_centroid, axis=(1, 2))\n",
    "    std_spectral_centroid = np.std(spectral_centroid, axis=(1, 2))\n",
    "    # print(\"spectral centroid: \", spectral_centroid.shape, avg_spectral_centroid.shape,\n",
    "    #       std_spectral_centroid.shape)\n",
    "    features_list.append(torch.tensor(avg_spectral_centroid, dtype=torch.float32).view(batch_size, 1))\n",
    "    features_list.append(torch.tensor(std_spectral_centroid, dtype=torch.float32).view(batch_size, 1))\n",
    "\n",
    "    # third family - spectral contrast\n",
    "    # spectral_contrast = librosa.feature.spectral_contrast(y=data)\n",
    "    # avg_spectral_contrast = np.mean(spectral_contrast, axis=(1, 2))\n",
    "    # std_spectral_contrast = np.std(spectral_contrast, axis=(1, 2))\n",
    "    # # print(\"spectral contrast: \", spectral_contrast.shape, avg_spectral_contrast.shape,\n",
    "    # #       std_spectral_contrast.shape)\n",
    "    # features_list.append(torch.tensor(avg_spectral_contrast, dtype=torch.float32).view(batch_size, 1))\n",
    "    # features_list.append(torch.tensor(std_spectral_contrast, dtype=torch.float32).view(batch_size, 1))\n",
    "\n",
    "    # forth family - log RMS\n",
    "    log_rms = librosa.feature.rms(y=librosa.amplitude_to_db(abs(data)))\n",
    "    avg_log_rms = np.mean(log_rms, axis=(1, 2))\n",
    "    std_log_rms = np.std(log_rms, axis=(1, 2))\n",
    "    # print(\"log RMS: \", log_rms.shape, avg_log_rms.shape, std_log_rms.shape)\n",
    "    features_list.append(torch.tensor(avg_log_rms, dtype=torch.float32).view(batch_size, 1))\n",
    "    features_list.append(torch.tensor(std_log_rms, dtype=torch.float32).view(batch_size, 1))\n",
    "\n",
    "    # # fifth family MFCC features\n",
    "    mfcc = librosa.feature.mfcc(y=data, n_mfcc=13)\n",
    "    # Padding first and second deltas\n",
    "    delta_mfcc = librosa.feature.delta(mfcc)\n",
    "    delta2_mfcc = librosa.feature.delta(mfcc, order=2)\n",
    "    zero_order_mfcc = np.mean(mfcc, axis=2)\n",
    "    first_order_mfcc = np.mean(delta_mfcc, axis=2)\n",
    "    second_order_mfcc = np.mean(delta2_mfcc, axis=2)\n",
    "\n",
    "    # print(\"MFCC: \", mfcc.shape, zero_order_mfcc.shape, first_order_mfcc.shape, second_order_mfcc.shape)\n",
    "    features_list.append(torch.tensor(zero_order_mfcc, dtype=torch.float32).view(batch_size, 13))\n",
    "    features_list.append(torch.tensor(first_order_mfcc, dtype=torch.float32).view(batch_size, 13))\n",
    "    features_list.append(torch.tensor(second_order_mfcc, dtype=torch.float32).view(batch_size, 13))\n",
    "\n",
    "    # sixth family - chroma\n",
    "    # chroma = librosa.feature.chroma_stft(y=data)\n",
    "    # avg_chroma = np.mean(chroma, axis=(1, 2))\n",
    "    # std_chroma = np.std(chroma, axis=(1, 2))\n",
    "    # # print(\"chroma: \", chroma.shape, avg_chroma.shape, std_chroma.shape)\n",
    "    # features_list.append(torch.tensor(avg_chroma, dtype=torch.float32).view(batch_size, 1))\n",
    "    # features_list.append(torch.tensor(std_chroma, dtype=torch.float32).view(batch_size, 1))\n",
    "\n",
    "    # seventh family - tonnetz\n",
    "    # tonnetz = librosa.feature.tonnetz(y=data)\n",
    "    # avg_tonnetz = np.mean(tonnetz, axis=(1, 2))\n",
    "    # std_tonnetz = np.std(tonnetz, axis=(1, 2))\n",
    "    # # print(\"tonnetz: \", tonnetz.shape, avg_tonnetz.shape, std_tonnetz.shape)\n",
    "    # features_list.append(torch.tensor(avg_tonnetz, dtype=torch.float32).view(batch_size, 1))\n",
    "    # features_list.append(torch.tensor(std_tonnetz, dtype=torch.float32).view(batch_size, 1))\n",
    "\n",
    "    # eigth - spectral flatness, rolloff, bandwidth\n",
    "    spectral_flatness = librosa.feature.spectral_flatness(y=data)\n",
    "    avg_spectral_flatness = np.mean(spectral_flatness, axis=(1, 2))\n",
    "    std_spectral_flatness = np.std(spectral_flatness, axis=(1, 2))\n",
    "    # print(\"spectral flatness: \", spectral_flatness.shape, avg_spectral_flatness.shape,\n",
    "    #       std_spectral_flatness.shape)\n",
    "    features_list.append(torch.tensor(avg_spectral_flatness, dtype=torch.float32).view(batch_size, 1))\n",
    "    features_list.append(torch.tensor(std_spectral_flatness, dtype=torch.float32).view(batch_size, 1))\n",
    "\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=data)\n",
    "    avg_spectral_rolloff = np.mean(spectral_rolloff, axis=(1, 2))\n",
    "    std_spectral_rolloff = np.std(spectral_rolloff, axis=(1, 2))\n",
    "    # print(\"spectral rolloff: \", spectral_rolloff.shape, avg_spectral_rolloff.shape,\n",
    "    #       std_spectral_rolloff.shape)\n",
    "    features_list.append(torch.tensor(avg_spectral_rolloff, dtype=torch.float32).view(batch_size, 1))\n",
    "    features_list.append(torch.tensor(std_spectral_rolloff, dtype=torch.float32).view(batch_size, 1))\n",
    "\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=data)\n",
    "    avg_spectral_bandwidth = np.mean(spectral_bandwidth, axis=(1, 2))\n",
    "    std_spectral_bandwidth = np.std(spectral_bandwidth, axis=(1, 2))\n",
    "    # print(\"spectral bandwidth: \", spectral_bandwidth.shape, avg_spectral_bandwidth.shape,\n",
    "    #       std_spectral_bandwidth.shape)\n",
    "    features_list.append(torch.tensor(avg_spectral_bandwidth, dtype=torch.float32).view(batch_size, 1))\n",
    "    features_list.append(torch.tensor(std_spectral_bandwidth, dtype=torch.float32).view(batch_size, 1))\n",
    "\n",
    "\n",
    "    features = torch.cat(features_list, dim=1)\n",
    "\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T19:16:06.897490700Z",
     "start_time": "2023-05-22T19:16:06.874674700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "def exctract_features_batch(wavs, batch_size):# extract features by batches\n",
    "    for batch in range(0, len(wavs), batch_size):\n",
    "        print(\"batch: \", batch)\n",
    "        batch_data = wavs[batch:batch + batch_size]\n",
    "        batch_features = exctract_feats(batch_data)\n",
    "        if batch == 0:\n",
    "            features = batch_features\n",
    "        else:\n",
    "            features = torch.cat((features, batch_features), dim=0)\n",
    "        print(\"features: \", features.shape)\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T19:32:01.061187200Z",
     "start_time": "2023-05-22T19:32:01.041376200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "n_data = np.asarray(data.tolist())\n",
    "t_data = torch.tensor(n_data, dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T19:24:26.871218700Z",
     "start_time": "2023-05-22T19:24:25.107115700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  0\n",
      "features:  torch.Size([100, 51])\n",
      "batch:  100\n",
      "features:  torch.Size([200, 51])\n"
     ]
    }
   ],
   "source": [
    "features = exctract_features_batch(t_data[:200], 100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T19:33:02.054101700Z",
     "start_time": "2023-05-22T19:32:24.393671600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "data": {
      "text/plain": "(51,)"
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T20:47:34.727927100Z",
     "start_time": "2023-05-22T20:47:34.699754400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.random' has no attribute 'randn'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [80]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m train_and_test_model(n_data[:\u001B[38;5;241m20\u001B[39m], labels[:\u001B[38;5;241m20\u001B[39m], features[:\u001B[38;5;241m20\u001B[39m]\u001B[38;5;241m.\u001B[39mnumpy())\n",
      "Input \u001B[1;32mIn [78]\u001B[0m, in \u001B[0;36mtrain_and_test_model\u001B[1;34m(wavs, labels, features, epochs, lr)\u001B[0m\n\u001B[0;32m     83\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m train_test_split(features, labels, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[0;32m     85\u001B[0m \u001B[38;5;66;03m# create and train the model\u001B[39;00m\n\u001B[1;32m---> 86\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mLogisticRegressor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munique\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     87\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain(X_train, y_train, epochs, lr)\n\u001B[0;32m     89\u001B[0m \u001B[38;5;66;03m# test the model\u001B[39;00m\n",
      "Input \u001B[1;32mIn [78]\u001B[0m, in \u001B[0;36mLogisticRegressor.__init__\u001B[1;34m(self, input_dim, num_classes)\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, input_dim, num_classes):\n\u001B[1;32m---> 36\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandn\u001B[49m(input_dim, num_classes)\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrandn(num_classes)\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'torch.random' has no attribute 'randn'"
     ]
    }
   ],
   "source": [
    "train_and_test_model(n_data[:20], labels[:20], features[:20].numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T19:44:39.469725Z",
     "start_time": "2023-05-22T19:44:39.225909300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [],
   "source": [
    "class LogisticRegressor:\n",
    "    def __init__(self, input_dim: int, num_classes: int):\n",
    "        self.weights = torch.randn(input_dim, num_classes, requires_grad=True)\n",
    "        self.bias = torch.randn(num_classes, requires_grad=True)\n",
    "\n",
    "    def forward(self, feats: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.mm(feats, self.weights) + self.bias\n",
    "\n",
    "    def backward(self, feats: torch.Tensor, output_scores: torch.Tensor, labels: torch.Tensor):\n",
    "        loss = torch.nn.functional.cross_entropy(output_scores, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.weights -= 0.01 * self.weights.grad\n",
    "            self.bias -= 0.01 * self.bias.grad\n",
    "\n",
    "            # zero the gradients after updating\n",
    "            self.weights.grad.zero_()\n",
    "            self.bias.grad.zero_()\n",
    "\n",
    "    def extract_feats(self, wavs: torch.Tensor):\n",
    "        # insert your feature extraction logic here\n",
    "        pass\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "\n",
    "def train_and_test_model(features: np.ndarray, labels: np.ndarray, epochs = 100):\n",
    "    # convert features and labels to torch tensors\n",
    "    features = torch.tensor(features, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    # split the features and labels into a training set and a testing set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "    print(y_train.shape)\n",
    "    # get the number of features from the feature data\n",
    "    input_dim = X_train.shape[1]\n",
    "\n",
    "    # get the number of unique classes from the labels\n",
    "    num_classes = len(torch.unique(labels))\n",
    "\n",
    "    # create the model\n",
    "    model = LogisticRegressor(input_dim, num_classes)\n",
    "\n",
    "    # training loop\n",
    "    print(epochs)\n",
    "    for epoch in range(epochs):\n",
    "        # forward pass\n",
    "        output_scores = model.forward(X_train)\n",
    "\n",
    "        # backward pass and optimization\n",
    "        model.backward(X_train, output_scores, y_train)\n",
    "\n",
    "        # print loss every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            loss = torch.nn.functional.cross_entropy(output_scores, y_train)\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}')\n",
    "\n",
    "    # testing the model\n",
    "    test_output_scores = model.forward(X_test)\n",
    "    _, predicted = torch.max(test_output_scores.data, 1)\n",
    "    correct = (predicted == y_test).sum().item()\n",
    "    test_accuracy = correct / len(y_test)\n",
    "    print(f'Test accuracy: {test_accuracy * 100:.2f}%')\n",
    "\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T20:45:21.392012300Z",
     "start_time": "2023-05-22T20:45:21.236747900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [51, 100]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [154]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m train_and_test_model(features[:\u001B[38;5;241m100\u001B[39m], labels[:\u001B[38;5;241m100\u001B[39m], \u001B[38;5;241m100\u001B[39m)\n",
      "Input \u001B[1;32mIn [146]\u001B[0m, in \u001B[0;36mtrain_and_test_model\u001B[1;34m(features, labels, epochs)\u001B[0m\n\u001B[0;32m     33\u001B[0m labels \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(labels, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong)\n\u001B[0;32m     35\u001B[0m \u001B[38;5;66;03m# split the features and labels into a training set and a testing set\u001B[39;00m\n\u001B[1;32m---> 36\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_test_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m42\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28mprint\u001B[39m(y_train\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m     38\u001B[0m \u001B[38;5;66;03m# get the number of features from the feature data\u001B[39;00m\n",
      "File \u001B[1;32mc:\\python38\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2559\u001B[0m, in \u001B[0;36mtrain_test_split\u001B[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001B[0m\n\u001B[0;32m   2556\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_arrays \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   2557\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAt least one array required as input\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 2559\u001B[0m arrays \u001B[38;5;241m=\u001B[39m \u001B[43mindexable\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43marrays\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2561\u001B[0m n_samples \u001B[38;5;241m=\u001B[39m _num_samples(arrays[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m   2562\u001B[0m n_train, n_test \u001B[38;5;241m=\u001B[39m _validate_shuffle_split(\n\u001B[0;32m   2563\u001B[0m     n_samples, test_size, train_size, default_test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.25\u001B[39m\n\u001B[0;32m   2564\u001B[0m )\n",
      "File \u001B[1;32mc:\\python38\\lib\\site-packages\\sklearn\\utils\\validation.py:443\u001B[0m, in \u001B[0;36mindexable\u001B[1;34m(*iterables)\u001B[0m\n\u001B[0;32m    424\u001B[0m \u001B[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001B[39;00m\n\u001B[0;32m    425\u001B[0m \n\u001B[0;32m    426\u001B[0m \u001B[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    439\u001B[0m \u001B[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001B[39;00m\n\u001B[0;32m    440\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    442\u001B[0m result \u001B[38;5;241m=\u001B[39m [_make_indexable(X) \u001B[38;5;28;01mfor\u001B[39;00m X \u001B[38;5;129;01min\u001B[39;00m iterables]\n\u001B[1;32m--> 443\u001B[0m \u001B[43mcheck_consistent_length\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    444\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32mc:\\python38\\lib\\site-packages\\sklearn\\utils\\validation.py:397\u001B[0m, in \u001B[0;36mcheck_consistent_length\u001B[1;34m(*arrays)\u001B[0m\n\u001B[0;32m    395\u001B[0m uniques \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(lengths)\n\u001B[0;32m    396\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 397\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    398\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound input variables with inconsistent numbers of samples: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    399\u001B[0m         \u001B[38;5;241m%\u001B[39m [\u001B[38;5;28mint\u001B[39m(l) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m lengths]\n\u001B[0;32m    400\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Found input variables with inconsistent numbers of samples: [51, 100]"
     ]
    }
   ],
   "source": [
    "model = train_and_test_model(features[:100], labels[:100], 100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T20:47:23.624825400Z",
     "start_time": "2023-05-22T20:47:23.510026400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 720x432 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming that your trained model is 'model'\n",
    "# and your features are ['MFCC', 'Spectral contrast', 'Chroma features', 'Tonnetz']\n",
    "\n",
    "# Extract weights\n",
    "weights = model.weights.detach().numpy()\n",
    "\n",
    "# Average weights across output classes\n",
    "average_weights = np.mean(weights, axis=1)\n",
    "\n",
    "# Feature names\n",
    "features = np.arange(0, 51)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(features, abs(average_weights))\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Average weight')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T19:58:19.188160600Z",
     "start_time": "2023-05-22T19:58:18.979739400Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
