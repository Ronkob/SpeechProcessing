{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcolab\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m drive\n\u001B[0;32m      2\u001B[0m drive\u001B[38;5;241m.\u001B[39mmount(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/content/drive\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T19:32:20.652370900Z",
     "start_time": "2023-08-21T19:32:20.605494500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/SPEECH/Code"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "!pip install jiwer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is the main part of the code. It makes models by phases"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python38\\lib\\site-packages\\torchaudio\\functional\\functional.py:576: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n",
      "c:\\python38\\lib\\site-packages\\torchaudio\\models\\decoder\\_ctc_decoder.py:62: UserWarning: The built-in flashlight integration is deprecated, and will be removed in future release. Please install flashlight-text. https://pypi.org/project/flashlight-text/ For the detail of CTC decoder migration, please see https://github.com/pytorch/audio/issues/3088.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tokens list:  ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ', '?']\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ', '?']\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import PreProcessing, PhaseOneModel, PhaseTwoModel, PhaseThreeModel, Evaluating\n",
    "import torch\n",
    "import wandb"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T13:30:59.785880600Z",
     "start_time": "2023-08-23T13:30:55.930311500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    learning_rate: float = 5e-4\n",
    "    epochs: int = 300\n",
    "    batch_size: int = 32\n",
    "    wandb_init: bool = False\n",
    "\n",
    "    hyperparams = {\n",
    "        \"n_cnn_layers\": 1,\n",
    "        \"n_rnn_layers\": 1,\n",
    "        \"rnn_dim\": 128,\n",
    "        \"n_class\": PreProcessing.NUM_CLASSES+1,\n",
    "        \"n_feats\": 128,\n",
    "        \"stride\": 2,\n",
    "        \"dropout\": 0.5,\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T13:30:59.833798500Z",
     "start_time": "2023-08-23T13:30:59.789993800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "def create_model(PhaseNumber, phase_model_class, config=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using {} device\".format(device))\n",
    "    architecture = PhaseNumber + 'Model'\n",
    "    # turn on and off wandb logging\n",
    "    # start a new wandb run to track this script\n",
    "    if config.wandb_init:\n",
    "        wandb.init(\n",
    "            # set the wandb project where this run will be logged\n",
    "            project=\"speechRecProj\",\n",
    "\n",
    "            # track hyperparameters and run metadata\n",
    "            config={\n",
    "                \"learning_rate\": config.learning_rate,\n",
    "                \"architecture\": architecture,\n",
    "                \"epochs\": config.epochs,\n",
    "                \"batch_size\": config.batch_size,\n",
    "                \"hyperparams\": config.hyperparams,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    test_dataloader = None\n",
    "    wavs, txts = PreProcessing.load_data(mode='train', data_path=PreProcessing.DATA_PATH)\n",
    "\n",
    "    # Now you can create a Dataset and DataLoader for your data\n",
    "    dataset = PreProcessing.AudioDatasetV3(wavs, txts)\n",
    "    train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.batch_size,\n",
    "                                                   shuffle=True,\n",
    "                                                   collate_fn=lambda x:\n",
    "                                                   PreProcessing.process_data(x))\n",
    "    wavs, txts = PreProcessing.load_data(mode='test', data_path=PreProcessing.DATA_PATH)\n",
    "    # test_dataset = PreProcessing.AudioDatasetV3(wavs, txts)\n",
    "    # test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=config.batch_size,\n",
    "    #                                               collate_fn=lambda x:\n",
    "    #                                               PreProcessing.process_data(x)\n",
    "    #                                               )\n",
    "    test_dataset = PreProcessing.AudioDatasetV2(wavs, txts)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=config.batch_size,\n",
    "                                                  shuffle=False)\n",
    "\n",
    "    # dataset = PreProcessing.AudioDatasetV2(wavs, txts)\n",
    "    # train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.batch_size,\n",
    "    #                                                shuffle=True)\n",
    "\n",
    "    model = PhaseThreeModel.PhaseThreeModel(config,\n",
    "                                            n_cnn_layers=config.hyperparams['n_cnn_layers'],\n",
    "                                            n_rnn_layers=config.hyperparams['n_rnn_layers'],\n",
    "                                            rnn_dim=config.hyperparams['rnn_dim'],\n",
    "                                            n_class=config.hyperparams['n_class'],\n",
    "                                            n_feats=config.hyperparams['n_feats'],\n",
    "                                            stride=config.hyperparams['stride'],\n",
    "                                            dropout=config.hyperparams['dropout'],\n",
    "                                            )\n",
    "    wavs, txts = PreProcessing.load_data(mode='train', data_path=PreProcessing.DATA_PATH)\n",
    "    dataset = PreProcessing.AudioDatasetV2(wavs, txts)\n",
    "    train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.batch_size,\n",
    "                                                   shuffle=False)\n",
    "    model = PhaseTwoModel.PhaseTwoModel()\n",
    "    return model, train_dataloader, test_dataloader, device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T13:30:59.849836400Z",
     "start_time": "2023-08-23T13:30:59.810274600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "model, train_dataloader, test_dataloader, device = create_model(\"PhaseThree\",\n",
    "                                                                    PhaseThreeModel.PhaseThreeModel,\n",
    "                                                                    config=Config(wandb_init=False))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T13:31:03.779936300Z",
     "start_time": "2023-08-23T13:30:59.833798500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 / 300  ( 0.0 %)\n",
      "[1,    10] loss: 14.089\n",
      "[1,    20] loss: 27.855\n",
      "Epoch:  1 / 300  ( 0.33333333333333337 %)\n",
      "[2,    10] loss: 12.652\n",
      "[2,    20] loss: 22.141\n",
      "Epoch:  2 / 300  ( 0.6666666666666667 %)\n",
      "[3,    10] loss: 7.785\n",
      "[3,    20] loss: 13.393\n",
      "Epoch:  3 / 300  ( 1.0 %)\n",
      "[4,    10] loss: 4.293\n",
      "[4,    20] loss: 8.359\n",
      "Epoch:  4 / 300  ( 1.3333333333333335 %)\n",
      "[5,    10] loss: 4.058\n",
      "[5,    20] loss: 8.047\n",
      "Epoch:  5 / 300  ( 1.6666666666666667 %)\n",
      "[6,    10] loss: 3.808\n",
      "[6,    20] loss: 7.473\n",
      "Epoch:  6 / 300  ( 2.0 %)\n",
      "[7,    10] loss: 3.511\n",
      "[7,    20] loss: 7.036\n",
      "Epoch:  7 / 300  ( 2.3333333333333335 %)\n",
      "[8,    10] loss: 3.412\n",
      "[8,    20] loss: 6.890\n",
      "Epoch:  8 / 300  ( 2.666666666666667 %)\n",
      "[9,    10] loss: 3.306\n",
      "[9,    20] loss: 6.685\n",
      "Epoch:  9 / 300  ( 3.0 %)\n",
      "[10,    10] loss: 3.279\n",
      "[10,    20] loss: 6.576\n",
      "Epoch:  10 / 300  ( 3.3333333333333335 %)\n",
      "[11,    10] loss: 3.376\n",
      "[11,    20] loss: 6.577\n",
      "Epoch:  11 / 300  ( 3.6666666666666665 %)\n",
      "[12,    10] loss: 3.307\n",
      "[12,    20] loss: 6.518\n",
      "Epoch:  12 / 300  ( 4.0 %)\n",
      "[13,    10] loss: 3.237\n",
      "[13,    20] loss: 6.452\n",
      "Epoch:  13 / 300  ( 4.333333333333334 %)\n",
      "[14,    10] loss: 3.248\n",
      "[14,    20] loss: 6.403\n",
      "Epoch:  14 / 300  ( 4.666666666666667 %)\n",
      "[15,    10] loss: 3.238\n",
      "[15,    20] loss: 6.467\n",
      "Epoch:  15 / 300  ( 5.0 %)\n",
      "[16,    10] loss: 3.185\n",
      "[16,    20] loss: 6.395\n",
      "Epoch:  16 / 300  ( 5.333333333333334 %)\n",
      "[17,    10] loss: 3.208\n",
      "[17,    20] loss: 6.367\n",
      "Epoch:  17 / 300  ( 5.666666666666666 %)\n",
      "[18,    10] loss: 3.200\n",
      "[18,    20] loss: 6.368\n",
      "Epoch:  18 / 300  ( 6.0 %)\n",
      "[19,    10] loss: 3.163\n",
      "[19,    20] loss: 6.358\n",
      "Epoch:  19 / 300  ( 6.333333333333334 %)\n",
      "[20,    10] loss: 3.157\n",
      "[20,    20] loss: 6.323\n",
      "Epoch:  20 / 300  ( 6.666666666666667 %)\n",
      "[21,    10] loss: 3.141\n",
      "[21,    20] loss: 6.315\n",
      "Epoch:  21 / 300  ( 7.000000000000001 %)\n",
      "[22,    10] loss: 3.160\n",
      "[22,    20] loss: 6.288\n",
      "Epoch:  22 / 300  ( 7.333333333333333 %)\n",
      "[23,    10] loss: 3.140\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [6]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m criterion \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mCTCLoss(blank\u001B[38;5;241m=\u001B[39mPreProcessing\u001B[38;5;241m.\u001B[39mBLANK_IDX)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m----> 2\u001B[0m PhaseThreeModel\u001B[38;5;241m.\u001B[39mtrain_model_phase_three(model, train_dataloader, criterion, device, test_dataloader, config\u001B[38;5;241m=\u001B[39mConfig(\n\u001B[0;32m      3\u001B[0m     wandb_init\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m))\n",
      "File \u001B[1;32m~\\Documents\\ACADEMY\\6th semester\\Speech Processing\\SPEECH\\Final Project\\Code\\PhaseThreeModel.py:180\u001B[0m, in \u001B[0;36mtrain_model_phase_three\u001B[1;34m(model, train_dataloader, criterion, device, test_dataloader, config)\u001B[0m\n\u001B[0;32m    178\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[0;32m    179\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch: \u001B[39m\u001B[38;5;124m\"\u001B[39m, epoch, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\"\u001B[39m, num_epochs, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m (\u001B[39m\u001B[38;5;124m\"\u001B[39m, epoch \u001B[38;5;241m/\u001B[39m num_epochs \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m100\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 180\u001B[0m     \u001B[43mrun_single_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    181\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meval_function\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    183\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs, num_epochs \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m50\u001B[39m):\n\u001B[0;32m    184\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch: \u001B[39m\u001B[38;5;124m\"\u001B[39m, epoch, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\"\u001B[39m, num_epochs, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m (\u001B[39m\u001B[38;5;124m\"\u001B[39m, epoch \u001B[38;5;241m/\u001B[39m num_epochs \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m100\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\Documents\\ACADEMY\\6th semester\\Speech Processing\\SPEECH\\Final Project\\Code\\PhaseThreeModel.py:215\u001B[0m, in \u001B[0;36mrun_single_epoch\u001B[1;34m(config, model, optimizer, scheduler, criterion, train_dataloader, device, epoch, eval_function)\u001B[0m\n\u001B[0;32m    212\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m    214\u001B[0m \u001B[38;5;66;03m# forward + backward + optimize\u001B[39;00m\n\u001B[1;32m--> 215\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    217\u001B[0m outputs_logsoft \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mlog_softmax(outputs, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m    219\u001B[0m input_lengths \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mas_tensor(input_lengths, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong)\n",
      "File \u001B[1;32mc:\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\Documents\\ACADEMY\\6th semester\\Speech Processing\\SPEECH\\Final Project\\Code\\PhaseThreeModel.py:58\u001B[0m, in \u001B[0;36mPhaseThreeModel.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 58\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcnn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     59\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrescnn_layers(x)\n\u001B[0;32m     60\u001B[0m     sizes \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39msize()\n",
      "File \u001B[1;32mc:\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mc:\\python38\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 463\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\python38\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    457\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    458\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 459\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    460\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CTCLoss(blank=PreProcessing.BLANK_IDX).to(device)\n",
    "PhaseThreeModel.train_model_phase_three(model, train_dataloader, criterion, device, test_dataloader, config=Config(\n",
    "    wandb_init=False))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T13:56:58.781440400Z",
     "start_time": "2023-08-23T13:33:55.137082600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# function ClickConnect(){\n",
    "# console.log(\"Working\");\n",
    "# document.querySelector(\"colab-toolbar-button#connect\").click()\n",
    "# }setInterval(ClickConnect,60000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-23T13:32:01.963017700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:256\"\n",
    "import torch\n",
    "import time\n",
    "import gc\n",
    "!pip install pynvml\n",
    "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def wait_until_enough_gpu_memory(min_memory_available, max_retries=10, sleep_time=5):\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(torch.cuda.current_device())\n",
    "\n",
    "    for _ in range(max_retries):\n",
    "        info = nvmlDeviceGetMemoryInfo(handle)\n",
    "        if info.free >= min_memory_available:\n",
    "            break\n",
    "        print(f\"Waiting for {min_memory_available} bytes of free GPU memory. Retrying in {sleep_time} seconds...\")\n",
    "        time.sleep(sleep_time)\n",
    "    else:\n",
    "        raise RuntimeError(f\"Failed to acquire {min_memory_available} bytes of free GPU memory after {max_retries} retries.\")\n",
    "\n",
    "# Usage example\n",
    "min_memory_available = 2 * 1024 * 1024 * 1024 * 5 # 10GB\n",
    "clear_gpu_memory()\n",
    "wait_until_enough_gpu_memory(min_memory_available)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
